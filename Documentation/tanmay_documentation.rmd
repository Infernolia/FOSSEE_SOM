
---
title: ''
bibliography: references.bib
output:
  pdf_document: default
  html_document:
    df_print: paged
---






```{css, echo=FALSE}
.center {
   text-align: center;
}

```
---

<div class="center">
  <h1>Self Organizing Maps</h1>
  <h7>R Programming, FOSSEE Team</h7>
</div>

---

```{R echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
#install.packages("tinytex")
#install.packages('rmarkdown')
#devtools::install_github('yihui/tinytex')
#tinytex::install_tinytex()
#library(tinytex)
#install.packages("DiagrammeR")
library(DiagrammeR)
#install.packages("diagram")
library(diagram)


```
# Table of Contents 

  - <a href="#lession1">Chapter 1: Introduction to SOM</a><br />
  - <a href="#lession2">Chapter 2: Algorithm</a><br />
  - <a href="#lession3">Chapter 3: Guided Tutorial in R</a><br />
  -- <a href="#t1">3.1: Initialization</a><br />
  -- <a href="#t2">3.2: Sampling</a><br />
  -- <a href="#t3">3.3: Competition</a><br />
  -- <a href="#t4">3.4: Cooperation</a><br />
  -- <a href="#t5">3.5: Adaptation</a><br />
  - <a href="#lession4">Chapter 4: Implementation</a><br />
    -- <a href="#t1">4.1: Data Generation</a><br />
  -- <a href="#t2">4.2: Initialization</a><br />
  -- <a href="#t3">4.3: Best Matching Unit</a><br />
  -- <a href="#t4">4.4:Training the SOM</a><br />
  - <a href="#lession5">Chapter 5: Optimization in R</a><br />
  - <a href="#lession6">Chapter 6: References</a><br />
    
    
    
<a id="lession1"></a>

## Chapter 1: Introduction to SOM 


Self organizing maps are a class of artificial neural networks based on competitive learning that helps to organize and understand high dimensional data by reducing the number of dimensions from a high dimensional space to a 2 D map. With SOM, clustering is performed by having several units compete for the current object. Once the data have been entered into the system, the newtwork of artificial neurons is trained by providing information about inputs. The weight vector of the unit is closest to the current object becomes the winning or active unit. During the training stage, the values for the input variables are gradually adjusted in an attempt to preserve neighborhood relationships that exist within the input data set. As it gets closer to the input object, the weights of the winning unit are adjusted as well as its neighbors.[@a1]

<div class="center">
![Figure 1: Kohonen model](model.png)


</div>  

#### Competitive Learning
The model utilizes using unsupervised learning to map the input through competitive learning in which the output neurons compete amongst themselves to be activated, with the result that only one is activated at any one time. Getting the Best Matching Unit is done by running through all wright vectors and calculating the distance from each weight to the sample vector. The weight with the shortest distance is the winner. There are numerous ways to determine the distance, however, the most commonly used method is the Euclidean Distance and/or Consine Distance.Due to the negative feedback connections between the neurons,  the neurons are forced to organise themselves which gave rise to the name Self Organizing Map (SOM).

<div class="center">
![Figure 2: Updating neighbourhood after finding BMU](neigh.png)
</div>  

<a id="lession2"></a>

## Chapter 2: Algorithm

```{R echo=TRUE, fig.width=25, fig.height=40, message=FALSE, warning=FALSE , include=FALSE}
library(DiagrammeR)
library(diagram)

png("flowchart.png", width = 500, height = 1200)
openplotmat(main = "")
 rx <- 0.3
 ry <- 0.03
pos <- coordinates(c(1,1,1,1,1,1,1,1 ), mx = -0.00001)
fromto <- matrix(ncol = 2, byrow = TRUE,  data = c(1, 2, 2, 3, 3,4,4,5,5,6,6,7,7,8))
nr <- nrow(fromto)

arrpos <- matrix(ncol = 2, nrow = nr) 
for (i in 1:nr)
{arrpos[i, ] <- straightarrow (to = pos[fromto[i, 2], ],
from = pos[fromto[i, 1], ],
lwd = 2, arr.pos = 0.6, arr.length = 0.5)
}
curvedarrow (from = pos[6, ], to = pos[3, ],lwd = 2, lty = 1)
textround(mid = pos[1,], radx = rx, rady = ry, lab = "Start", cex = 2, shadow.col = "black")
textdiamond(mid = pos[2,], radx = rx, rady = ry, lab = "Initialization", cex = 2, shadow.col = "lightblue")
textrect(mid = pos[3,], radx = rx, rady = ry, lab = "Sampling", cex = 2, shadow.col = "darkred")
textrect(mid = pos[4,], radx = rx, rady = ry, lab = "Competition", cex = 2, shadow.col = "darkred")
textrect(mid = pos[5,], radx = rx, rady = ry, lab = "Cooperation", cex = 2, shadow.col = "darkred")
textrect(mid = pos[6,], radx = rx, rady = ry, lab = "Adaptation", cex = 2, shadow.col = "darkred")
textrect(mid = pos[7,], radx = rx, rady = ry, lab = "Convergence", cex = 2, shadow.col = "darkred")
textround(mid = pos[8,], radx = rx, rady = ry, lab = "End", cex = 2, shadow.col = "black")

dev.off()

```

<div class="center">
![Figure 3: Flowchart](flowchart.png)
</div>
### Steps: 

- Initialization - Create a grid of neurons and randomly initialize weights.
- Sampling- Select a random row (vector) from input data.
- Competition- Neurons fight to become the Best Matching Unit which is determined using the discriminant function.
- Cooperation- The winning neuron determines the spatial location of a topological neighbourhood of excited neurons which will cooperate.
- Adaptation- Weights are adjusted with respect to winning neuron, such that a similar input pattern is enhanced.
- We will go back to step 2 and keep repeating the process till the map stops changing or convergence is achieved.

<a id="lession3"></a>

## Chapter 3: Guided Tutorial in R


<a id="t1"></a>

### 3.1: Initialization 

Create a grid of neurons and randomly initialize weights. The neurons are represented by weight vectors of same dimensions as input. The random numbers are generated using the rnorm function which generates  random numbers in the range -1 to 1.

Code snippet:


```{R echo=TRUE, message=FALSE, warning=FALSE}
#Let's create a matrix of 10 rows and 5 columns
t <- matrix(data = rnorm(50),  nrow = (10), ncol = 5)
t
```

<a id="t2"></a>

### 3.2:  Sampling

Select a random row (vector) from input data. The sampling is done using the sample() function in R which retrieves an input row.


```{R echo=TRUE, message=FALSE, warning=FALSE}
i <- sample(t, 1, replace = F)
i
```

<a id="t3"></a>

### 3.3:  Competition

Neurons fight to become the Best Matching Unit which is determined using the discriminant function.Here our discriminant function is Euclidean distance given by the formula:


<body>
\begin{equation}
 d\left( x,y\right) = \sqrt {\sum _{i=1}^{n}  \left( y_{i}- x_{i}\right)^2 } 
 \end{equation}
 where $x$ and $y$ are the two points in n dimensional space  and $x_{i}$ and $y_{i}$ are the vectors representing their positions between which the Euclidean distance is to be calculated.
 <center> Euclidean distance formula</center><br><br><br><br>
</body>




```{R echo=TRUE, message=FALSE, warning=FALSE}

#Lets make a function to calculate euclidean distance.
euclidean_distance <- function(x, y) {
  ret <- sum((x - y)^2)
  return(ret)
}

#Let's run this on a sample input

euclidean_distance(2,4)
```


The Best Matching Unit is the neuron which is closest to the input vector. The discriminant function is used to calculate this distance between all the neurons' weights and the input vector.

<div class="center">
![Figure 4: Discriminant function calculation](dis.png)
</div>


```{R echo=TRUE, message=FALSE, warning=FALSE}

# x is a single input row of data and input_grid is the grid
BMU <- function(x, input_grid) { 
  distance <- 0
  min_distance <- 10000000 # Setting high min dist value
  min_ind <- -1 # Setting random min_ind value
  for (e in 1:nrow(input_grid)) # Iterating through grid
  {
    distance <- euclidean_distance(x, input_grid[e, ]) # euclidean_distance distance
    if (distance < min_distance) {
      min_distance <- distance # Updating min distance for winning unit
      min_ind <- e # Updating winning neuron
    }
  }
  return(min_ind-1) #returns index of BMU
}
```



<a id="t4"></a>

### 3.4:  Cooperation

The winning neuron determines the spatial location of a topological neighbourhood of excited neurons which will cooperate.


<body>
\begin{equation}
 influence = exp(-(distance^{2}) / (2 * (radius^{2}))) 
 \end{equation}
 where $distance$ is the lateral distance between neurons in the grid and  $radius$ is the radius of the neighbourhood over which influence is to be calculated.
 <center> Neighbourhood influence calculation formula</center><br><br>
</body>

```{R echo=TRUE, message=FALSE, warning=FALSE}
# Defining a function to calculate the neighbourhood influence using the radius of neighbourhood and lateral distance.
influence_calculation <- function(distance, radius) {
  ret <- exp(-(distance^2) / (2 * (radius^2)))
  return(ret)
}

# Calculating sample neighbourhood for lateral distance 2 and radius 4.
influence_calculation(2,4)
```

<a id="t5"></a>

### 3.5:  Adaptation

Weights are adjusted with respect to winning neuron, such that a similar input pattern is enhanced.


<body>
\begin{equation}
 new\_radius = radius * exp(-current\_iteration / time\_constant) 
 \end{equation}
 where $radius$ is the initial radius of neighbourhood, $current\_iteration$ is the iteration of data sampling that we are currently on and  $time\_constant$ is the time constant which is incremented at each iteration, when the SOM gets updated.
 <center> Radius decay formula</center><br><br>
 <\body>
 
 
```{R echo=TRUE, message=FALSE, warning=FALSE}

#Function for the decaying radius for a given iteration current_iteration
decay_radius_function <- function(radius, current_iteration, time_constant) {
  ret <- radius * exp(-current_iteration / time_constant)
  return(ret)
}

# Calculate radius of neighborhood at a iteration 4, with radius 3 and at the 4th iteration
decay_radius_function(3,4,4)

```

 <body>
 \begin{equation}
 new\_lateral\_distance = learning\_rate * exp(-current\_iteration / n\_iteration)
 \end{equation}
 where $learning\_rate$ is the old learning rate to be updated, $current\_iteration$ is the iteration of data sampling that we are currently on  and  $n\_iteration$ is the total number of iterations the SOM is trained over.
 <center> Learning rate decay formula</center><br><br>
</body>




```{R echo=TRUE, message=FALSE, warning=FALSE}

#Function for the decaying learning rate
decay_learning_rate <- function(learning_rate, current_iteration, n_iteration) {
  ret <- learning_rate * exp(-current_iteration / n_iteration)
  return(ret)
}

#Calculating the learning rate of model at the 3rd iteration out of a total of 100 iterations and initial learning rate of 0.1.
decay_learning_rate(0.1,3,100)

```


<a id="lession4"></a>

## Chapter 4:  Implementation

<a id="e1"></a>

### 4.1: Data Generation

For this tutorial, we will demonstrate the working of SOM on a given dataset of 3 dimensions. We will load this dataset from the working directory. We will also import the necessary libraries.

<B>Code Snippet</B>

```{R echo=TRUE, message=FALSE, warning=FALSE}

set.seed(222)
library(dplyr)

# 1) Reading the data and scaling it
data <- read.csv("binary.csv", header = T)
X <- scale(data[, -1])
data <- X
```

<a id="e2"></a>


### 4.2: Initialization

The SOM is in its essence a grid of neurons, each neuron containing a weight vector and a position i,j in the grid. We begin by assigning random values for the initial weight vectors w. The dimensions of the weight vector are equal to the number of input dimensions.



<div class="center">
![Figure 5: Weights matrix](W.png)
</div> 

<B>Code Snippet</B>

```{R echo=TRUE, message=FALSE, warning=FALSE}

#Now lets initialize the weights of the neural network.
#Creating a 4x4 neural network with 3 dimensions to match the input.

# -----------------------------------------------------
# This is Step 1 of the Algorithm: Initialization
# -----------------------------------------------------

create_grid <- function(n,p) {
  ret <- matrix(data = rnorm(n * p), nrow = n, ncol = p)
  return(ret)
}
grid <- create_grid(16,3)
grid
```



<a id="e3"></a>
### 4.3: Best Matching Unit 

The SOM works using competitive learning which selects a best matching unit at each iteration using the discriminant function value closest to the randomly sampled input vector. 



<B>Code Snippet</B>

```{R echo=TRUE, message=FALSE, warning=FALSE}

# -----------------------------------------------------
# This is Step 3 of the Algorithm: Competition
# -----------------------------------------------------

# euclidean_distance function
euclidean_distance <- function(x, y) {
  ret <- sum((x - y)^2)
  return(ret)
}

# Function to return winning neuron
BMU <- function(x, input_grid) { 
  distance <- 0
  min_distance <- 10000000 # Setting high min dist value
  min_ind <- -1 # Setting random min_ind value
  for (e in 1:nrow(input_grid)) # Iterating through grid
  {
    distance <- euclidean_distance(x, input_grid[e, ]) # euclidean_distance distance
    if (distance < min_distance) {
      min_distance <- distance # Updating min distance for winning unit
      min_ind <- e # Updating winning neuron
    }
  }
  return(min_ind-1) #returns index of BMU
}

```

<a id="e4">  </a>


### 4.4: Training the SOM.

The SOM follows the algorithm mentioned above to fit the training data till the map stops changing or in other words till the model converges. 

<B>Code Snippet</B>

```{R echo=TRUE, message=FALSE, warning=FALSE}

# -----------------------------------------------------
# This is Step 5 of the Algorithm: Adaptation
# -----------------------------------------------------

# Defining the updation function first.
# 1) Decaying radius function
decay_radius_function <- function(radius, current_iteration, time_constant) {
  ret <- radius * exp(-current_iteration / time_constant)
  return(ret)

}

# -----------------------------------------------------
# This is Step 4 of the Algorithm: Cooperation
# -----------------------------------------------------


# 2) Decaying learning rate
decay_learning_rate <- function(learning_rate, current_iteration, n_iteration) {
  ret <- learning_rate * exp(-current_iteration / n_iteration)
  return(ret)
}

# 3) A function to calculate influence over neighboring neurons
influence_calculation <- function(distance, radius) {
  ret <- exp(-(distance^2) / (2 * (radius^2)))
  return(ret)
}


SOM <- function(x, input_grid) {
  
  
# Defining the training parameters.
  
   n_iteration <- 400 # Defining number of iterations
  initial_learning_rate <- 0.05 # Defining initial learning rate
  initial_radius <- 3 # Defining initial radius
  time_constant <- n_iteration / log(initial_radius) # Initializing time constant
  lateral_distance_points=expand.grid(1:sqrt(nrow(input_grid)),1:sqrt(nrow(input_grid)))#Initialising physical locations of neurons to figure out lateral distance.
  rows=sqrt(nrow(input_grid)) #The square grid is used here - so taking the number of rows as square root of number of entries in the grid.
  n_epochs=10 #Defining the number of epochs.
  for(ne in 1:n_epochs)
  {
    print(ne)
    old_grid=input_grid
    for (i in 1:n_iteration) # Looping through for training
    {
      sample_input_row <- as.vector(unlist(x[sample(1:nrow(x), size = 1, replace = F), ])) # Selecting random input row from given data set
      new_radius <- decay_radius_function(initial_radius, i, time_constant) # Decaying radius
      new_learning_rate <- max(decay_learning_rate(initial_learning_rate, i, n_iteration), 0.01) # Decaying learning rate
      index_temp <- BMU(sample_input_row, input_grid) # Finding best matching unit for given input row
      index_new=c((as.integer(index_temp/rows))+1,(index_temp%%rows)+1) #Converting a 1D co-ordinate to a 2D co-ordinate for finding lateral distance on the map.
      lateral_distance=sqrt(rowSums(sweep(lateral_distance_points,2,index_new)^2)) #Finding Euclidean distance between the given best matching units and all units on the map.
      rn=which(lateral_distance<=new_radius) #Finding neurons that are within the radius of the winning unit.
      inf=influence_calculation(lateral_distance[rn],new_radius) #Calculating the influence of the winning neuron on neighbours.
      diff_grid=(sweep(input_grid[rn,],2,sample_input_row))*-1 #A temporary matrix that stores the difference between the data point and the weights of the winning neuron & neighbours.
      updated_weights=new_learning_rate*inf*diff_grid #The updating operation on the winning and neighbouring neurons.
      input_grid[rn,]=input_grid[rn,]+updated_weights #Now updating those grid entries that are either the winning neuron or its neighbours.
      if(isTRUE(all.equal(old_grid,input_grid)))
      {
        print(i)
        print("Converged")
      }
    }
  }
  return(input_grid) #Returning the updated SOM weights.
}
start <- Sys.time()
gridSOM=SOM(data,grid)
end <- Sys.time()
gridSOM

time_taken <- end - start
print(time_taken)

```


<a id="lession5"></a>

## Chapter 5: Optimization in R

If you wish to explore further optimization of the SOM code, try running the below code cells and compare the running time of two approaches. The method of optimization here is vectorization.

```{R echo=TRUE, message=FALSE, warning=FALSE}
BMU_Vectorised <- function(x, input_grid) { 
  dist_mtrx=rowSums(sweep(input_grid,2,x)^2) #Calculating the distance of this row from all the neurons using matrix operations.
  min_ind=which.min(dist_mtrx) #Finding the location of the neuron with the minimum distance.
  return (min_ind-1) #Returning the zero-indexed value of the winning neuron.
}
```

```{R echo=TRUE, message=FALSE, warning=FALSE}

#Fastest BMU Implementation using vectorisation.
#x is a single row of data and input_grid is the grid


SOM <- function(x, input_grid) {
  
  
# Defining the training parameters.
  
   n_iteration <- 400 # Defining number of iterations
  initial_learning_rate <- 0.05 # Defining initial learning rate
  initial_radius <- 3 # Defining initial radius
  time_constant <- n_iteration / log(initial_radius) # Initializing time constant
  lateral_distance_points=expand.grid(1:sqrt(nrow(input_grid)),1:sqrt(nrow(input_grid)))#Initialising physical locations of neurons to figure out lateral distance.
  rows=sqrt(nrow(input_grid)) #The square grid is used here - so taking the number of rows as square root of number of entries in the grid.
  n_epochs=10 #Defining the number of epochs.
  for(ne in 1:n_epochs)
  {
    print(ne)
    old_grid=input_grid
    for (i in 1:n_iteration) # Looping through for training
    {
      sample_input_row <- as.vector(unlist(x[sample(1:nrow(x), size = 1, replace = F), ])) # Selecting random input row from given data set
      new_radius <- decay_radius_function(initial_radius, i, time_constant) # Decaying radius
      new_learning_rate <- max(decay_learning_rate(initial_learning_rate, i, n_iteration), 0.01) # Decaying learning rate
      index_temp <- BMU_Vectorised(sample_input_row, input_grid) # Finding best matching unit for given input row
      index_new=c((as.integer(index_temp/rows))+1,(index_temp%%rows)+1) #Converting a 1D co-ordinate to a 2D co-ordinate for finding lateral distance on the map.
      lateral_distance=sqrt(rowSums(sweep(lateral_distance_points,2,index_new)^2)) #Finding Euclidean distance between the given best matching units and all units on the map.
      rn=which(lateral_distance<=new_radius) #Finding neurons that are within the radius of the winning unit.
      inf=influence_calculation(lateral_distance[rn],new_radius) #Calculating the influence of the winning neuron on neighbours.
      diff_grid=(sweep(input_grid[rn,],2,sample_input_row))*-1 #A temporary matrix that stores the difference between the data point and the weights of the winning neuron & neighbours.
      updated_weights=new_learning_rate*inf*diff_grid #The updating operation on the winning and neighbouring neurons.
      input_grid[rn,]=input_grid[rn,]+updated_weights #Now updating those grid entries that are either the winning neuron or its neighbours.
      if(isTRUE(all.equal(old_grid,input_grid)))
      {
        print(i)
        print("Converged")
      }
    }
  }
  return(input_grid) #Returning the updated SOM weights.
}
start <- Sys.time()
gridSOM=SOM(data,grid)
end <- Sys.time()
gridSOM


time_taken <- end - start
print(time_taken)

```


nocite: | 
  @a2, @a3, @a4, @a5, @a6, @a7, @a8, @a9

<a id="lession6"> </a>

## Chapter 6: References
